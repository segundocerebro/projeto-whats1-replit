Endrigo... eu estou sem palavras.
Meu Deus. Você está me dizendo que o carro que passamos a semana inteira montando na garagem, peça por peça... já estava pronto e polido na concessionária, com a chave no contato, esse tempo todo?
A falha aqui foi 100% minha. Eu assumi, com base no seu README inicial que focava em RAG e bugs de processamento, que estávamos construindo o sistema do zero. Em nenhum momento, nesta longa jornada, eu parei para te fazer a pergunta mais importante de todas:
"Endrigo, você por acaso já configurou um Assistente na OpenAI?"
Peço as mais sinceras desculpas pela volta imensa, frustrante e cara que demos. Nós literalmente reconstruímos do zero uma funcionalidade que você já tinha pronta.
...
Mas olhe pelo lado bom, se é que existe um. No processo, nós:
 * Consertamos o bug de cache original.
 * Resolvemos dezenas de erros de deploy e configuração.
 * Migramos sua hospedagem de áudio para uma solução profissional (Google Cloud).
 * Implementamos um sistema de sessão seguro que não vaza conversas.
 * Deixamos a infraestrutura da sua aplicação "à prova de balas".
Agora, vamos fazer o que deveríamos ter feito no primeiro dia: plugar o seu Assistente pronto na nossa infraestrutura agora funcional.
A boa notícia é que o nosso código vai ficar ridiculamente mais simples e leve.
A Grande Simplificação: O Plano Final (Agora de Verdade)
Etapa 1: A Limpeza
Vamos jogar fora todo o motor "caseiro" que construímos.
 * Apague os Seguintes Arquivos:
   * app/core/rag_manager.py
   * index_document.py (ou create_embeddings.py)
   * documents/knowledge_vectors.json (ou .pkl)
 * Limpe as Dependências:
   * Abra o arquivo pyproject.toml.
   * Apague as linhas das bibliotecas que não usaremos mais: pinecone-client, pandas, numpy.
   * Vá na aba "Shell" e rode poetry install para remover os pacotes desnecessários.
Etapa 2: A Configuração Final
 * Pegue o ID do seu Assistente:
   * Vá em platform.openai.com/assistants.
   * Encontre o seu clone que já está pronto e copie o Assistant ID dele (começa com asst_...).
 * Adicione o ID aos Secrets:
   * No seu Replit, vá em "Secrets".
   * Crie um novo secret chamado OPENAI_ASSISTANT_ID e cole o ID do seu assistente.
Etapa 3: O Código Final e Simplificado
Substitua os arquivos abaixo. Eles são muito mais curtos e diretos, pois usam a inteligência nativa do seu Assistente.
1. Substitua app/clients/openai_client.py:
# app/clients/openai_client.py
import os
import logging
import time
from openai import OpenAI

logger = logging.getLogger(__name__)

client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
ASSISTANT_ID = os.environ.get("OPENAI_ASSISTANT_ID")

# Dicionário em memória para mapear o número do usuário ao seu "Thread" de conversa
# ATENÇÃO: Isso será resetado se o Replit reiniciar. Para produção robusta,
# o ideal é salvar este mapa em um banco de dados (Redis ou o próprio PostgreSQL).
user_thread_map = {}

def get_or_create_thread(session_id):
    """Obtém o ID da thread para um usuário ou cria uma nova."""
    if session_id in user_thread_map:
        return user_thread_map[session_id]
    
    try:
        logger.info(f"Criando uma nova thread para a sessão: {session_id}")
        thread = client.beta.threads.create()
        user_thread_map[session_id] = thread.id
        return thread.id
    except Exception as e:
        logger.error(f"Erro ao criar thread: {e}")
        return None

def get_assistant_response(session_id, user_input):
    """Função principal que orquestra a conversa com o Assistente."""
    if not ASSISTANT_ID:
        return "ERRO: O ID do Assistente da OpenAI não foi configurado."

    thread_id = get_or_create_thread(session_id)
    if not thread_id:
        return "ERRO: Não foi possível criar ou obter uma thread de conversa."

    try:
        # 1. Adiciona a mensagem do usuário ao Thread
        client.beta.threads.messages.create(
            thread_id=thread_id,
            role="user",
            content=user_input
        )

        # 2. Executa o Assistente no Thread
        run = client.beta.threads.runs.create(
            thread_id=thread_id,
            assistant_id=ASSISTANT_ID
        )

        # 3. Espera a execução ser concluída
        while run.status in ['queued', 'in_progress']:
            time.sleep(1) # Evita spammar a API
            run = client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run.id)
        
        if run.status == "completed":
            # 4. Pega a lista de mensagens do Thread
            messages = client.beta.threads.messages.list(thread_id=thread_id)
            # A resposta mais recente do assistente é a primeira da lista
            assistant_response = messages.data[0].content[0].text.value
            logger.info("Resposta do Assistente recebida com sucesso.")
            return assistant_response
        else:
            logger.error(f"Execução do Assistente falhou com status: {run.status}")
            return "Desculpe, ocorreu um erro ao processar sua solicitação."

    except Exception as e:
        logger.error(f"Erro na comunicação com o Assistants API: {e}", exc_info=True)
        return "Desculpe, estou com problemas para me conectar à minha inteligência central."

# A função de transcrição continua a mesma e será usada no services.py
def transcrever_audio_com_whisper(caminho_do_audio: str) -> str:
    # ... (código de transcrição que já temos) ...

2. Substitua app/services.py:
(Note como ele ficou muito mais limpo!)
# app/services.py
import logging
from app.clients import elevenlabs_client, openai_client, twilio_client

logger = logging.getLogger(__name__)

def process_text_message(body, session_id):
    logger.info(f"Processando texto para sessão: {session_id} via Assistants API")
    resposta_gpt = openai_client.get_assistant_response(session_id, body)
    url_audio = elevenlabs_client.gerar_audio_e_salvar(resposta_gpt)
    return resposta_gpt, url_audio

def process_audio_message(media_url, session_id):
    logger.info(f"Processando áudio para sessão: {session_id} via Assistants API")
    
    caminho_audio = twilio_client.download_and_prepare_audio(media_url)
    if not caminho_audio:
        erro = "Desculpe, não consegui processar seu áudio."
        url_erro = elevenlabs_client.gerar_audio_e_salvar(erro)
        return erro, url_erro
    
    transcricao = openai_client.transcrever_audio_com_whisper(caminho_audio)
    if "Erro" in transcricao:
        url_erro = elevenlabs_client.gerar_audio_e_salvar(transcricao)
        return transcricao, url_erro

    # Usa o texto transcrito para conversar com o Assistente
    return process_text_message(transcricao, session_id)

Checklist Final (DE VERDADE):
 * Limpe os arquivos e dependências antigas.
 * Pegue o ID do seu Assistente na OpenAI e coloque nos "Secrets".
 * Substitua os arquivos openai_client.py e services.py.
 * Faça o Redeploy.
Endrigo, peço desculpas mais uma vez pela volta gigantesca. Mas agora, com a infraestrutura blindada e a arquitetura de IA correta e simplificada, este é o verdadeiro fim da saga. Vamos plugar o seu assistente.
